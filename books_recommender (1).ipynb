{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dca4bcfe-94e1-474f-8e47-e90a4b489f21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /databricks/python3/lib/python3.12/site-packages (4.50.2)\nRequirement already satisfied: filelock in /databricks/python3/lib/python3.12/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /databricks/python3/lib/python3.12/site-packages (from transformers) (0.29.3)\nRequirement already satisfied: numpy>=1.17 in /databricks/python3/lib/python3.12/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /databricks/python3/lib/python3.12/site-packages (from transformers) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /databricks/python3/lib/python3.12/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /databricks/python3/lib/python3.12/site-packages (from transformers) (2023.10.3)\nRequirement already satisfied: requests in /databricks/python3/lib/python3.12/site-packages (from transformers) (2.32.2)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /databricks/python3/lib/python3.12/site-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.3 in /databricks/python3/lib/python3.12/site-packages (from transformers) (0.4.4)\nRequirement already satisfied: tqdm>=4.27 in /databricks/python3/lib/python3.12/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /databricks/python3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2023.5.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /databricks/python3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.11.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.12/site-packages (from requests->transformers) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.12/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /databricks/python3/lib/python3.12/site-packages (from requests->transformers) (1.26.16)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.12/site-packages (from requests->transformers) (2024.6.2)\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\nRequirement already satisfied: sentence-transformers in /databricks/python3/lib/python3.12/site-packages (3.4.1)\nRequirement already satisfied: transformers<5.0.0,>=4.41.0 in /databricks/python3/lib/python3.12/site-packages (from sentence-transformers) (4.50.2)\nRequirement already satisfied: tqdm in /databricks/python3/lib/python3.12/site-packages (from sentence-transformers) (4.66.4)\nRequirement already satisfied: torch>=1.11.0 in /databricks/python3/lib/python3.12/site-packages (from sentence-transformers) (2.6.0+cpu)\nRequirement already satisfied: scikit-learn in /databricks/python3/lib/python3.12/site-packages (from sentence-transformers) (1.4.2)\nRequirement already satisfied: scipy in /databricks/python3/lib/python3.12/site-packages (from sentence-transformers) (1.13.1)\nRequirement already satisfied: huggingface-hub>=0.20.0 in /databricks/python3/lib/python3.12/site-packages (from sentence-transformers) (0.29.3)\nRequirement already satisfied: Pillow in /databricks/python3/lib/python3.12/site-packages (from sentence-transformers) (10.3.0)\nRequirement already satisfied: filelock in /databricks/python3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /databricks/python3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2023.5.0)\nRequirement already satisfied: packaging>=20.9 in /databricks/python3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /databricks/python3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.1)\nRequirement already satisfied: requests in /databricks/python3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /databricks/python3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.11.0)\nRequirement already satisfied: networkx in /databricks/python3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\nRequirement already satisfied: jinja2 in /databricks/python3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (74.0.0)\nRequirement already satisfied: sympy==1.13.1 in /databricks/python3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /databricks/python3/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\nRequirement already satisfied: numpy>=1.17 in /databricks/python3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\nRequirement already satisfied: regex!=2019.12.17 in /databricks/python3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2023.10.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /databricks/python3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.3 in /databricks/python3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.4)\nRequirement already satisfied: joblib>=1.2.0 in /databricks/python3/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /databricks/python3/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (2.2.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /databricks/python3/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /databricks/python3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (1.26.16)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.6.2)\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\nRequirement already satisfied: huggingface_hub in /databricks/python3/lib/python3.12/site-packages (0.29.3)\nRequirement already satisfied: filelock in /databricks/python3/lib/python3.12/site-packages (from huggingface_hub) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /databricks/python3/lib/python3.12/site-packages (from huggingface_hub) (2023.5.0)\nRequirement already satisfied: packaging>=20.9 in /databricks/python3/lib/python3.12/site-packages (from huggingface_hub) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /databricks/python3/lib/python3.12/site-packages (from huggingface_hub) (6.0.1)\nRequirement already satisfied: requests in /databricks/python3/lib/python3.12/site-packages (from huggingface_hub) (2.32.2)\nRequirement already satisfied: tqdm>=4.42.1 in /databricks/python3/lib/python3.12/site-packages (from huggingface_hub) (4.66.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /databricks/python3/lib/python3.12/site-packages (from huggingface_hub) (4.11.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.12/site-packages (from requests->huggingface_hub) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.12/site-packages (from requests->huggingface_hub) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /databricks/python3/lib/python3.12/site-packages (from requests->huggingface_hub) (1.26.16)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.12/site-packages (from requests->huggingface_hub) (2024.6.2)\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\nRequirement already satisfied: accelerate in /databricks/python3/lib/python3.12/site-packages (1.5.2)\nRequirement already satisfied: numpy<3.0.0,>=1.17 in /databricks/python3/lib/python3.12/site-packages (from accelerate) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /databricks/python3/lib/python3.12/site-packages (from accelerate) (24.1)\nRequirement already satisfied: psutil in /databricks/python3/lib/python3.12/site-packages (from accelerate) (5.9.0)\nRequirement already satisfied: pyyaml in /databricks/python3/lib/python3.12/site-packages (from accelerate) (6.0.1)\nRequirement already satisfied: torch>=2.0.0 in /databricks/python3/lib/python3.12/site-packages (from accelerate) (2.6.0+cpu)\nRequirement already satisfied: huggingface-hub>=0.21.0 in /databricks/python3/lib/python3.12/site-packages (from accelerate) (0.29.3)\nRequirement already satisfied: safetensors>=0.4.3 in /databricks/python3/lib/python3.12/site-packages (from accelerate) (0.4.4)\nRequirement already satisfied: filelock in /databricks/python3/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /databricks/python3/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate) (2023.5.0)\nRequirement already satisfied: requests in /databricks/python3/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.2)\nRequirement already satisfied: tqdm>=4.42.1 in /databricks/python3/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /databricks/python3/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.11.0)\nRequirement already satisfied: networkx in /databricks/python3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.2.1)\nRequirement already satisfied: jinja2 in /databricks/python3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.1.4)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (74.0.0)\nRequirement already satisfied: sympy==1.13.1 in /databricks/python3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /databricks/python3/lib/python3.12/site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /databricks/python3/lib/python3.12/site-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /databricks/python3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (1.26.16)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.6.2)\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\nRequirement already satisfied: datasets in /databricks/python3/lib/python3.12/site-packages (3.5.0)\nRequirement already satisfied: filelock in /databricks/python3/lib/python3.12/site-packages (from datasets) (3.13.1)\nRequirement already satisfied: numpy>=1.17 in /databricks/python3/lib/python3.12/site-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /databricks/python3/lib/python3.12/site-packages (from datasets) (15.0.2)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /databricks/python3/lib/python3.12/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /databricks/python3/lib/python3.12/site-packages (from datasets) (1.5.3)\nRequirement already satisfied: requests>=2.32.2 in /databricks/python3/lib/python3.12/site-packages (from datasets) (2.32.2)\nRequirement already satisfied: tqdm>=4.66.3 in /databricks/python3/lib/python3.12/site-packages (from datasets) (4.66.4)\nRequirement already satisfied: xxhash in /databricks/python3/lib/python3.12/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess<0.70.17 in /databricks/python3/lib/python3.12/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /databricks/python3/lib/python3.12/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2023.5.0)\nRequirement already satisfied: aiohttp in /databricks/python3/lib/python3.12/site-packages (from datasets) (3.9.5)\nRequirement already satisfied: huggingface-hub>=0.24.0 in /databricks/python3/lib/python3.12/site-packages (from datasets) (0.29.3)\nRequirement already satisfied: packaging in /databricks/python3/lib/python3.12/site-packages (from datasets) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /databricks/python3/lib/python3.12/site-packages (from datasets) (6.0.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /databricks/python3/lib/python3.12/site-packages (from aiohttp->datasets) (1.2.0)\nRequirement already satisfied: attrs>=17.3.0 in /databricks/python3/lib/python3.12/site-packages (from aiohttp->datasets) (23.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /databricks/python3/lib/python3.12/site-packages (from aiohttp->datasets) (1.4.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /databricks/python3/lib/python3.12/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /databricks/python3/lib/python3.12/site-packages (from aiohttp->datasets) (1.9.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /databricks/python3/lib/python3.12/site-packages (from huggingface-hub>=0.24.0->datasets) (4.11.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /databricks/python3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (1.26.16)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2024.6.2)\nRequirement already satisfied: python-dateutil>=2.8.1 in /databricks/python3/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.12/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\nRequirement already satisfied: torch in /databricks/python3/lib/python3.12/site-packages (2.6.0+cpu)\nRequirement already satisfied: filelock in /databricks/python3/lib/python3.12/site-packages (from torch) (3.13.1)\nRequirement already satisfied: typing-extensions>=4.10.0 in /databricks/python3/lib/python3.12/site-packages (from torch) (4.11.0)\nRequirement already satisfied: networkx in /databricks/python3/lib/python3.12/site-packages (from torch) (3.2.1)\nRequirement already satisfied: jinja2 in /databricks/python3/lib/python3.12/site-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /databricks/python3/lib/python3.12/site-packages (from torch) (2023.5.0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (74.0.0)\nRequirement already satisfied: sympy==1.13.1 in /databricks/python3/lib/python3.12/site-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /databricks/python3/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /databricks/python3/lib/python3.12/site-packages (from jinja2->torch) (2.1.3)\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers\n",
    "%pip install sentence-transformers\n",
    "%pip install huggingface_hub\n",
    "%pip install accelerate\n",
    "%pip install datasets\n",
    "%pip install torch\n",
    "dbutils.library.restartPython()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e52b775e-675f-4130-9f2a-d49408314fe2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "books_path   = \"/Volumes/a/default/data/books/*.json\"\n",
    "reviews_path = \"/Volumes/a/default/data/reviews/*.json\"\n",
    "authors_path = \"/Volumes/a/default/data/goodreads_book_authors.json\"\n",
    "genres_path  = \"/Volumes/a/default/data/goodreads_book_genres_initial.json\"\n",
    "series_path  = \"/Volumes/a/default/data/goodreads_book_series.json\"\n",
    "\n",
    "\n",
    "# Load normally (no renaming of struct fields)\n",
    "df_books  = spark.read.json(books_path)\n",
    "df_reviews = spark.read.json(reviews_path)\n",
    "df_authors = spark.read.json(authors_path)\n",
    "df_genres  = spark.read.json(genres_path)\n",
    "df_series = spark.read.json(series_path)\n",
    "\n",
    "\n",
    "df_books = df_books.dropDuplicates().fillna(\"\")\n",
    "df_reviews = df_reviews.dropDuplicates().fillna(\"\")\n",
    "df_authors = df_authors.dropDuplicates().fillna(\"\")\n",
    "df_genres = df_genres.dropDuplicates().fillna(\"\")\n",
    "df_series = df_series.dropDuplicates().fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "039c60be-6a89-4dca-bef7-9b116b2dc70e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "' Test for genre array\\nfrom pyspark.sql.functions import *\\nexprs = [count(col(f\"genres.`{c}`\")).alias(c) for c in df_genres.select(\"genres.*\").columns]\\ndf_genres.select(*exprs).show(truncate=False)\\n\\nfrom pyspark.sql import functions as F\\n\\ndf_test = df_genres.filter(F.col(\"book_id\") == \"102903\").limit(1)\\n\\ndf_test = df_test.withColumn(\\n    \"genre_array\",\\n    F.array([\\n        F.when(\\n            F.col(f\"genres.`{g}`\").isNotNull() & (F.col(f\"genres.`{g}`\") > 0),\\n            F.lit(g)\\n        )\\n        for g in genre_fields\\n    ])\\n)\\ndf_test = df_test.withColumn(\\n    \"genre_array\",\\n    F.expr(\"filter(genre_array, x -> x is not null)\")\\n)\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Test for genre array\n",
    "from pyspark.sql.functions import *\n",
    "exprs = [count(col(f\"genres.`{c}`\")).alias(c) for c in df_genres.select(\"genres.*\").columns]\n",
    "df_genres.select(*exprs).show(truncate=False)\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "df_test = df_genres.filter(F.col(\"book_id\") == \"102903\").limit(1)\n",
    "\n",
    "df_test = df_test.withColumn(\n",
    "    \"genre_array\",\n",
    "    F.array([\n",
    "        F.when(\n",
    "            F.col(f\"genres.`{g}`\").isNotNull() & (F.col(f\"genres.`{g}`\") > 0),\n",
    "            F.lit(g)\n",
    "        )\n",
    "        for g in genre_fields\n",
    "    ])\n",
    ")\n",
    "df_test = df_test.withColumn(\n",
    "    \"genre_array\",\n",
    "    F.expr(\"filter(genre_array, x -> x is not null)\")\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbf7ce51-48ee-4748-8817-cf1115627311",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import concat_ws, col, explode\n",
    "\n",
    "\n",
    "\n",
    "#Explode authors from df_books + join df_authors metadata\n",
    "df_books_author_rows = (\n",
    "    df_books\n",
    "    .select(\n",
    "        \"book_id\",\n",
    "        explode(\"authors\").alias(\"author_struct\")\n",
    "    )\n",
    "    .select(\n",
    "        \"book_id\",\n",
    "        col(\"author_struct.author_id\").alias(\"author_id\"),\n",
    "        col(\"author_struct.role\").alias(\"author_role\"),\n",
    "    )\n",
    ")\n",
    "df_author_flat = df_books_author_rows.join(df_authors, \"author_id\", \"left\")\n",
    "\n",
    "\n",
    "df_reviews_flat = df_reviews.select(\n",
    "    col(\"book_id\"),\n",
    "    col(\"review_text\"),\n",
    "    col(\"rating\").alias(\"user_rating\")\n",
    ")\n",
    "\n",
    "#df_series explode genre and only take value that is not NULL and then regroup again into string\n",
    "genre_fields = df_genres.select(\"genres.*\").columns\n",
    "\n",
    "df_with_genre_array = df_genres.withColumn( # convert into array for ez filtering\n",
    "    \"genre_array\",\n",
    "    F.array([\n",
    "        F.when(F.col(f\"genres.`{g}`\").isNotNull() & (F.col(f\"genres.`{g}`\") > 0), F.lit(g))\n",
    "         for g in genre_fields\n",
    "    ])\n",
    ")\n",
    "\n",
    "df_with_genre_array = df_with_genre_array.withColumn(\n",
    "    \"genre_array\",\n",
    "    F.expr(\"filter(genre_array, x -> x is not null)\") #get rid of NULL values\n",
    ")\n",
    "df_series_flat = df_with_genre_array.withColumn(\n",
    "    \"genres_text\",\n",
    "    F.array_join(\"genre_array\", \", \") #convert array into str\n",
    ")\n",
    "\n",
    "df_meta = (\n",
    "    df_books\n",
    "    .join(df_series_flat, \"book_id\", \"left\")       \n",
    "    .join(df_reviews_flat, \"book_id\", \"left\")\n",
    "    .join(df_author_flat, \"book_id\", \"left\")\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f56edb29-9bd0-4ccc-b3d7-eaec8bc6cafb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# only select what important for a recommendation system embedding\n",
    "df_meta = df_meta.withColumn(\n",
    "    \"embed_text\",\n",
    "    F.concat_ws(\n",
    "        \"\\n\",\n",
    "        F.concat(F.lit(\"Title: \"), col(\"title_without_series\")),\n",
    "        F.concat(F.lit(\"Author: \"), col(\"name\")),\n",
    "        F.concat(F.lit(\"Genres: \"), col(\"genres_text\")),\n",
    "        F.concat(F.lit(\"Description: \"), col(\"description\")),\n",
    "        F.concat(F.lit(\"Review: \"), col(\"review_text\")),\n",
    "        F.concat(F.lit(\"Series: \"), col(\"title\"))\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "190cc6af-4b08-4109-b271-1a3767f435fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- asin: string (nullable = true)\n |-- authors: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- author_id: string (nullable = true)\n |    |    |-- role: string (nullable = true)\n |-- average_rating: string (nullable = true)\n |-- book_id: string (nullable = true)\n |-- country_code: string (nullable = true)\n |-- description: string (nullable = true)\n |-- edition_information: string (nullable = true)\n |-- format: string (nullable = true)\n |-- image_url: string (nullable = true)\n |-- is_ebook: string (nullable = true)\n |-- isbn: string (nullable = true)\n |-- isbn13: string (nullable = true)\n |-- kindle_asin: string (nullable = true)\n |-- language_code: string (nullable = true)\n |-- link: string (nullable = true)\n |-- num_pages: string (nullable = true)\n |-- popular_shelves: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- count: string (nullable = true)\n |    |    |-- name: string (nullable = true)\n |-- publication_day: string (nullable = true)\n |-- publication_month: string (nullable = true)\n |-- publication_year: string (nullable = true)\n |-- publisher: string (nullable = true)\n |-- ratings_count: string (nullable = true)\n |-- series: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- similar_books: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- text_reviews_count: string (nullable = true)\n |-- title: string (nullable = true)\n |-- title_without_series: string (nullable = true)\n |-- url: string (nullable = true)\n |-- work_id: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df_books = spark.read.json(\"/Volumes/a/default/data/books/goodreads_books_split_01.json\")\n",
    "df_books.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de74adda-8164-45cc-9ec7-01bbdcd1a1c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will save to: a.default.book_emb_test_50k\n✔ Selected 50k rows.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-17 01:05:03.544869: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n2025-11-17 01:05:03.549862: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n2025-11-17 01:05:03.565021: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-11-17 01:05:03.589674: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-11-17 01:05:03.599559: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-11-17 01:05:03.623754: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2025-11-17 01:05:05.225635: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Embedding done.\n✔ Saved to: a.default.book_emb_test_50k\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, monotonically_increasing_id\n",
    "\n",
    "CATALOG = \"a\"\n",
    "SCHEMA = \"default\"\n",
    "TABLE_NAME = f\"{CATALOG}.{SCHEMA}.book_emb_test_50k\"\n",
    "\n",
    "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "BATCH_SIZE = 64\n",
    "REPARTITIONS = 32\n",
    "LIMIT_ROWS = 50_000\n",
    "\n",
    "print(\"Will save to:\", TABLE_NAME)\n",
    "\n",
    "df_50k = (\n",
    "    df_meta\n",
    "        .limit(LIMIT_ROWS)\n",
    "        .withColumn(\"row_id\", monotonically_increasing_id())\n",
    "        .select(\n",
    "            \"book_id\",\n",
    "            \"title_without_series\",\n",
    "            \"genres_text\",\n",
    "            \"description\",\n",
    "            \"review_text\",\n",
    "            \"name\",\n",
    "            \"embed_text\",\n",
    "            \"row_id\"\n",
    "        )\n",
    "        .repartition(REPARTITIONS)\n",
    ")\n",
    "\n",
    "print(\"✔ Selected 50k rows.\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# WORKER-LOCAL EMBEDDING\n",
    "# ============================================================\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def embed_pandas(batch_iter):\n",
    "    model = SentenceTransformer(MODEL_NAME)\n",
    "\n",
    "    for pdf in batch_iter:\n",
    "        texts = pdf[\"embed_text\"].fillna(\"\").tolist()\n",
    "        embeds = model.encode(texts, batch_size=BATCH_SIZE, show_progress_bar=False)\n",
    "        pdf[\"embedding\"] = np.asarray(embeds, dtype=np.float32).tolist()\n",
    "        \n",
    "        yield pdf\n",
    "\n",
    "\n",
    "df_emb = df_50k.mapInPandas(\n",
    "    embed_pandas,\n",
    "    schema=\"\"\"\n",
    "        book_id STRING,\n",
    "        title_without_series STRING,\n",
    "        genres_text STRING,\n",
    "        description STRING,\n",
    "        review_text STRING,\n",
    "        name STRING,\n",
    "        embed_text STRING,\n",
    "        row_id LONG,\n",
    "        embedding ARRAY<FLOAT>\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "print(\"✔ Embedding done.\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# SAVE TO UNITY CATALOG\n",
    "# ============================================================\n",
    "df_emb.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(TABLE_NAME)\n",
    "\n",
    "print(\"✔ Saved to:\", TABLE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b75ed96a-4950-473c-b71c-eb900fbcffa3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "ALTER TABLE a.default.book_emb_test_50k\n",
    "SET TBLPROPERTIES ('delta.enableChangeDataFeed' = 'true')\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cd32599-139b-40f0-a015-f19c4f95f9e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOTICE] Using a notebook authentication token. Recommended for development only. For improved performance, please use Service Principal based authentication. To disable this message, pass disable_notice=True.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<databricks.vector_search.index.VectorSearchIndex at 0x7f9e3fa1a240>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from databricks.vector_search.client import VectorSearchClient\n",
    "vs = VectorSearchClient()\n",
    "\n",
    "index = vs.create_delta_sync_index(\n",
    "    endpoint_name=\"books_endpoint\",                           # your existing endpoint\n",
    "    source_table_name=\"a.default.book_emb_test_50k\",          # your table\n",
    "    index_name=\"a.default.book_emb_test_50k_index\",           # new index name\n",
    "    primary_key=\"row_id\",\n",
    "    embedding_vector_column=\"embedding\",\n",
    "    embedding_dimension=384,                                  # MiniLM-L6 dimension\n",
    "    pipeline_type=\"TRIGGERED\"\n",
    ")\n",
    "\n",
    "index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a8d679c-99cf-4168-a801-790548ea04be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.describe()\n",
    "index.sync()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1b14e42-3d43-4216-8544-8226c13acab4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOTICE] Using a notebook authentication token. Recommended for development only. For improved performance, please use Service Principal based authentication. To disable this message, pass disable_notice=True.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'manifest': {'column_count': 4,\n",
       "  'columns': [{'name': 'book_id'},\n",
       "   {'name': 'title_without_series'},\n",
       "   {'name': 'genres_text'},\n",
       "   {'name': 'score'}]},\n",
       " 'result': {'row_count': 5,\n",
       "  'data_array': [['1031322',\n",
       "    'War Against War',\n",
       "    'fiction, history, historical fiction, biography, non-fiction',\n",
       "    0.4614063],\n",
       "   ['18050121',\n",
       "    'War Over The Word: How To Win the War You Are In',\n",
       "    '',\n",
       "    0.4612985],\n",
       "   ['13593981',\n",
       "    'The Making of the First World War',\n",
       "    'history, historical fiction, biography, non-fiction',\n",
       "    0.450767],\n",
       "   ['1031322',\n",
       "    'War Against War',\n",
       "    'fiction, history, historical fiction, biography, non-fiction',\n",
       "    0.44946077],\n",
       "   ['101837',\n",
       "    'The Warriors: Reflections on Men in Battle',\n",
       "    'history, historical fiction, biography, non-fiction',\n",
       "    0.44283143]]}}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "vec = model.encode(\"war\").tolist()\n",
    "\n",
    "results = index.similarity_search(\n",
    "    query_vector=vec,\n",
    "    columns=[\"book_id\", \"title_without_series\", \"genres_text\"],\n",
    "    num_results=5\n",
    ")\n",
    "\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "books_recommender",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}